{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Scrape URLs of all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from scrape_urls import *\n",
    "\n",
    "# Commented out because it starts a long process\n",
    "# urls_2019 = get_months(2019)\n",
    "\n",
    "# with open('url_19.txt', 'w') as fout:\n",
    "#     fout.write('\\n'.join(urls_2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Once URLs were scraped they were fed into Scrapy which got the actual articles.\n",
    "\n",
    "Article data was loaded into MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Pull text data from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "\n",
    "db = client.proj4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = list(db.all_19.find({}, {'title':1, 'text':1, 'date':1, 'url':1, '_id':0}))\n",
    "\n",
    "tds_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Clean text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To clean the raw text, I removed all non-standard characters like emojis, made all words lowercase, dropped punctuation, and removed all numeric characters.\n",
    "\n",
    "This left the documents in a much more standardized state where they were easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Cleaning Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def del_emoji(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove emoji and make lowercase\n",
    "    clean_text = del_emoji(text).lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', clean_text)\n",
    "    \n",
    "    # remove digits\n",
    "    clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "tds_data['text'] = tds_data.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In addition to cleaning the article's text, I dropped all the articles that were less than 500 words in length. This eliminated a number of articles that were not parsed properly during scraping, and left me with slightly longer articles that had more well-defined topics.\n",
    "\n",
    "The URLs of articles all had a '?' followed by some kind of hex key. I needed to remove this key from the end to be able to merge with the claps data which I scraped later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Drop articles with less than 500 words\n",
    "tds_data = tds_data[tds_data.text.apply(lambda x: len(x.split(' '))>=500)]\n",
    "\n",
    "tds_data['url'] = tds_data.url.apply(lambda x: x.split('?')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I stemmed and lemmatized all of my documents to standardize the forms of all the words and find more commonality between documents.\n",
    "\n",
    "I was hoping to use Spacy for lemmatization because I find their lemmatizer to be more consistent than NLTK, but I did not have time to implement that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def stemmify(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "\n",
    "def lemmafy(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tds_data['stemmed'] = tds_data.text.apply(stemmify)\n",
    "tds_data['lemmad'] = tds_data.text.apply(lemmafy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "# Pull claps data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I had to go back and scrape claps (likes) for each of my articles after the fact, so I pulled them in from a separate JSON and merged it with the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./scrapy/tds/tds/claps.json', 'r') as j:\n",
    "    clap_json = json.loads(j.read())\n",
    "    \n",
    "claps = pd.DataFrame(clap_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Cleaning claps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I did a bit of work cleaning up the clap data, filling Nulls and trimming the URLs to be able to merge with the text.\n",
    "\n",
    "Articles with >1000 claps are listed as having #.#K claps, so I converted the K to a multiple of 1000 with a short function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "claps['claps'] = claps.claps.fillna('0')\n",
    "claps['claps'] = claps.claps.apply(lambda x: x.strip())\n",
    "claps['url'] = claps.url.apply(lambda x: x.split('?')[0])\n",
    "\n",
    "def convert_claps(clap_str):\n",
    "    \n",
    "    try:\n",
    "        claps = int(clap_str)\n",
    "    except:\n",
    "        if 'K' in clap_str:\n",
    "            claps = int(float(clap_str[:-1])*1000)\n",
    "        else:\n",
    "            claps = 0\n",
    "    return claps\n",
    "\n",
    "claps['claps'] = claps.claps.apply(convert_claps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Merge text and claps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tds_data = tds_data.merge(claps, on='url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Send data back to mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The dataframe needs to be converted to a list of dicts before it can be imported into mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tds_dicts = tds_data.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# db.final_cleaned.insert_many(tds_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
