{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "nov_2019 = pd.read_csv('On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2019_11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Calculate average taxi in time for each destination airport and average taxi out time for ORD (origin airport)\n",
    "\n",
    "> **NOTE: taxi times were eventually left out of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "taxi_in_means = nov_2019.groupby('Dest')['TaxiIn'].mean()\n",
    "taxi_in_means = taxi_in_means.apply(lambda x: pd.to_timedelta(x, unit='m').round('min'))\n",
    "taxi_in_means.name = 'MeanTaxiIn'\n",
    "taxi_in_means = taxi_in_means.reset_index()\n",
    "\n",
    "ORD_taxi_out = pd.to_timedelta(nov_2019[nov_2019.Origin=='ORD'].TaxiOut.mean(), unit='m').round('min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Restrict data down to departures from Ohare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ohare = nov_2019[(nov_2019.Origin=='ORD')].reset_index(drop=True)\n",
    "\n",
    "# Drop cancelled flights\n",
    "ohare = ohare[ohare.Cancelled == 0]\n",
    "\n",
    "# Drop flights that were diverted and never got to destination\n",
    "ohare = ohare[ohare.Diverted==0]\n",
    "    \n",
    "# Reformat times so I can parse to datetime\n",
    "def add_zeros(s):\n",
    "    while len(s) < 4:\n",
    "        s = '0'+s\n",
    "    return s\n",
    "\n",
    "# Times are recorded with midnight=2400 instead of 0000 which is how python needs them\n",
    "time_cols = ['CRSDepTime', 'DepTime', 'CRSArrTime', 'ArrTime']\n",
    "for col in time_cols:\n",
    "    \n",
    "    ohare[col] = ohare[col].astype(int).astype(str)\n",
    "    ohare[col] = ohare[col].replace('2400', '0000')\n",
    "    ohare[col] = ohare[col].apply(add_zeros)\n",
    "    \n",
    "ohare = ohare.loc[:, :'FirstDepTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Cut out unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flights = ohare.loc[:, ['FlightDate', 'Reporting_Airline', 'Flight_Number_Reporting_Airline',\n",
    "                        'Tail_Number', 'Dest', 'CRSDepTime', \n",
    "                        'DepTime', 'DepDelayMinutes', 'CRSArrTime', 'ArrTime',\n",
    "                        'ArrDelayMinutes', 'Distance']]\n",
    "\n",
    "# Combine with average taxi times calculated above\n",
    "flights = flights.merge(taxi_in_means, how='inner', on='Dest')\n",
    "flights['MeanTaxiOut'] = ORD_taxi_out\n",
    "\n",
    "flights.columns = ['date', 'airline', 'flight_no', 'tail_number', 'dest', 'sched_depart', 'depart', 'dep_delay',\n",
    "                   'sched_arr', 'arr', 'arr_delay', 'distance', 'mean_taxi_in', 'mean_taxi_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Parse dates/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flights['sched_depart'] = pd.to_datetime(flights.date + ' ' + flights.sched_depart)\n",
    "flights['depart'] = pd.to_datetime(flights.date + ' ' + flights.depart)\n",
    "flights['sched_arr'] = pd.to_datetime(flights.date + ' ' + flights.sched_arr)\n",
    "flights['arr'] = pd.to_datetime(flights.date + ' ' + flights.arr)\n",
    "\n",
    "flights['dep_delay'] = pd.to_timedelta(flights.dep_delay, unit='m')\n",
    "flights['arr_delay'] = pd.to_timedelta(flights.arr_delay, unit='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Fix dates/times to be in UTC Unix time\n",
    "\n",
    "Times were recorded in the local time zone of each airport, so I converted all the times to UNIX time for standardization and easier calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Import airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "airports_all = pd.read_csv('AIRPORT_MASTER_RECORD.csv')\n",
    "airports_all = airports_all.groupby('AIRPORT').first().reset_index()\n",
    "\n",
    "# Filter down to airports in the Uniter States because I am only analyzing domestic flights\n",
    "airports_all = airports_all[airports_all.AIRPORT_COUNTRY_NAME=='United States']\n",
    "\n",
    "# Select only the destination airports in my dataset\n",
    "airport_data = airports_all[airports_all.AIRPORT.isin(flights.dest)]\n",
    "\n",
    "# Add ORD\n",
    "airport_data = airport_data.append(airports_all[airports_all.AIRPORT=='ORD'])\n",
    "\n",
    "airport_data = airport_data.set_index('AIRPORT')\n",
    "airport_data.to_csv('airports.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Save time difference to UTC for each airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flights['ord_to_utc'] = pd.to_timedelta(airport_data.loc['ORD', 'UTC_LOCAL_TIME_VARIATION']/100, unit='h')\n",
    "flights['dest_to_utc'] = pd.to_timedelta(airport_data.loc[flights.dest, 'UTC_LOCAL_TIME_VARIATION'].values/100, unit='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flights['sched_dep_unix'] = (flights.sched_depart.dt.round('h') - flights.ord_to_utc).astype(int)/10**9\n",
    "flights['sched_arr_unix'] = (flights.sched_arr.dt.round('h') - flights.dest_to_utc).astype(int)/10**9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Filter out destinations that have <100 arrivals\n",
    "\n",
    "Destination airports with fewer than 100 flights arriving do not provide enough data to be meaningful, and create noise within the data. By removing them I am able to focus modeling on busier airports in more populous areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dest_mask = flights.groupby('dest').flight_no.count() > 100\n",
    "flights = flights.set_index('dest').loc[dest_mask[dest_mask].index].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Make target column\n",
    "\n",
    "My target column is a binary late arrival flag if a flight arrives any time after its scheduled arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flights['arr_late'] = (flights.arr_delay > pd.to_timedelta(0)).astype(int)\n",
    "flights.to_csv('flights.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Get weather data from DarkSky\n",
    "\n",
    "I can use the Dark Sky API to fetch weather at each airport on an hourly basis. This weather data can be matched with the hour nearest to the arrival of a given flight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create queries for every airport on every day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range('nov 1 2019', 'nov 30 2019').astype(str)\n",
    "dates = dates+'T00:00:00'\n",
    "\n",
    "with open('darksky_api_key.txt') as keyfile:\n",
    "    darksky_key = keyfile.readline().strip()\n",
    "\n",
    "# Build a DataFrame to store the query URL for each airport on each date\n",
    "weather_queries = pd.DataFrame(columns=['Airport', 'Date', 'Query'])\n",
    "\n",
    "# Retreive the latitude and longitude of each airport to build queries\n",
    "for airport in flights.dest.unique():\n",
    "    lat = airport_data.loc[airport, 'LATITUDE']\n",
    "    lon = airport_data.loc[airport, 'LONGITUDE']\n",
    "    for date in dates:\n",
    "        q_dict = {\n",
    "            'Airport':airport,\n",
    "            'Date':date,\n",
    "            'Query':f'https://api.darksky.net/forecast/{darksky_key}/{lat},{lon},{date}?exclude=currently'\n",
    "        }\n",
    "        weather_queries = weather_queries.append(q_dict, ignore_index=True)\n",
    "        \n",
    "# Add queries to find weather at my origin airport (ORD) for each date\n",
    "for date in dates:\n",
    "    q_dict = {\n",
    "        'Airport':'ORD',\n",
    "        'Date':date,\n",
    "        'Query':f'https://api.darksky.net/forecast/{darksky_key}/{lat},{lon},{date}?exclude=currently'\n",
    "    }\n",
    "    weather_queries = weather_queries.append(q_dict, ignore_index=True)\n",
    "    \n",
    "weather_queries = weather_queries.set_index('Airport')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Helper function to read weather for a given airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(airport):\n",
    "    df = pd.DataFrame()\n",
    "    for i, q in enumerate(weather_queries.loc[airport, 'Query'], 1):\n",
    "        r = requests.get(q)\n",
    "        if r.status_code != 200:\n",
    "            print(f'Error on request {q}')\n",
    "            return None\n",
    "        hourly = r.json()['hourly']['data']\n",
    "        df = df.append(hourly)\n",
    "    df['airport'] = airport\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loop through airports and read all weather\n",
    "\n",
    "> **NOTE: the DarkSky API only allows you to make 1,000 queries a day with a free API key, so if you intend on making more than 1,000 calls to the API at once you will need to register payment information with DarkSky**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get weather data for arrival destinations\n",
    "# weather_data = pd.DataFrame()\n",
    "# for i, airport in enumerate(flight_data.Dest.unique()):\n",
    "#     weather_data = weather_data.append(get_weather(airport))\n",
    "#     weather_data.to_csv('weather_data.csv')\n",
    "#     time.sleep(2)\n",
    "\n",
    "# Get weather data for ORD\n",
    "# ord_weather = get_weather('ORD')\n",
    "# weather_data = weather_data.append(ord_weather)\n",
    "# weather_data.to_csv('weather_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Send data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas.io.sql as psql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('weather_data.csv')\n",
    "flight_data = pd.read_csv('flights.csv')\n",
    "airport_data = pd.read_csv('airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('aws_pgsql_pw.txt') as pwfile:\n",
    "    password = pwfile.read()\n",
    "\n",
    "params = {\n",
    "    'host':'IP ADDRESS',\n",
    "    'user':'USER',\n",
    "    'dbname':'flight_delays',\n",
    "    'port':5432,\n",
    "    'password':password\n",
    "}\n",
    "\n",
    "connection_string = f'postgres://{params[\"user\"]}:{params[\"host\"]}@{params[\"host\"]}:{params[\"port\"]}/{params[\"dbname\"]}'\n",
    "\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I use the `to_sql` function only to send the schema of each of my tables to my Postgres server. The contents of these tables are copied into the database directly from csv files using the Postgres shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flight_data.iloc[:0].to_sql('flights', engine, index=False)\n",
    "weather_data.iloc[:0].to_sql('weather', engine, index=False)\n",
    "airport_data.iloc[:0].to_sql('airports', engine, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
